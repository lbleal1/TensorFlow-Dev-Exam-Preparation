{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"name":"2 - binary_clf - sarcasm data.ipynb","provenance":[{"file_id":"https://github.com/lmoroney/dlaicourse/blob/master/TensorFlow%20In%20Practice/Course%203%20-%20NLP/Course%203%20-%20Week%202%20-%20Lesson%202.ipynb","timestamp":1607763272648}],"collapsed_sections":[],"toc_visible":true},"kernelspec":{"display_name":"Python 3","name":"python3"}},"cells":[{"cell_type":"code","metadata":{"id":"4gs9htvM7n_x"},"source":["# Run this to ensure TensorFlow 2.x is used\n","try:\n","  # %tensorflow_version only exists in Colab.\n","  %tensorflow_version 2.x\n","except Exception:\n","  pass"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"XYYDvoskkE61"},"source":["import json\n","import tensorflow as tf\n","\n","from tensorflow.keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.preprocessing.sequence import pad_sequences"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"cQg-5HbsJHRc"},"source":["# 1 Data"]},{"cell_type":"markdown","metadata":{"id":"WrrD6itZJOV3"},"source":["## Getting and Extracting the Data"]},{"cell_type":"code","metadata":{"id":"BQVuQrZNkPn9"},"source":["!wget --no-check-certificate \\\n","    https://storage.googleapis.com/laurencemoroney-blog.appspot.com/sarcasm.json \\\n","    -O /tmp/sarcasm.json\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"oaLaaqhNkUPd"},"source":["# Extracting sentences and labels from a json file\n","\n","with open(\"/tmp/sarcasm.json\", 'r') as f:\n","    datastore = json.load(f)\n","\n","sentences = []\n","labels = []\n","\n","for item in datastore:\n","    sentences.append(item['headline'])\n","    labels.append(item['is_sarcastic'])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"djEuX3QOKskf"},"source":["## Creating train and test sentences and labels"]},{"cell_type":"code","metadata":{"id":"S1sD-7v0kYWk"},"source":["# Splitting training and testing datasets - both on sentences and labels\n","training_size = 20000\n","\n","training_sentences = sentences[0:training_size]\n","testing_sentences = sentences[training_size:]\n","\n","training_labels = labels[0:training_size]\n","testing_labels = labels[training_size:]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"clFJVRJtJfoe"},"source":["## Preprocessing - Tokenizer and Pad Sequences (both train and test data)"]},{"cell_type":"code","metadata":{"id":"0eJSTTYnkJQd"},"source":["vocab_size = 10000\n","embedding_dim = 16\n","max_length = 100\n","trunc_type='post'\n","padding_type='post'\n","oov_tok = \"<OOV>\"\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"3u8UB0MCkZ5N"},"source":["tokenizer = Tokenizer(num_words=vocab_size, oov_token=oov_tok)\n","tokenizer.fit_on_texts(training_sentences)\n","\n","word_index = tokenizer.word_index\n","\n","training_sequences = tokenizer.texts_to_sequences(training_sentences)\n","training_padded = pad_sequences(training_sequences, maxlen=max_length, padding=padding_type, truncating=trunc_type)\n","\n","testing_sequences = tokenizer.texts_to_sequences(testing_sentences)\n","testing_padded = pad_sequences(testing_sequences, maxlen=max_length, padding=padding_type, truncating=trunc_type)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"GrAlWBKf99Ya"},"source":["# Need this block to get it to work with TensorFlow 2.x\n","# convert to numpy array\n","\n","import numpy as np\n","training_padded = np.array(training_padded)\n","training_labels = np.array(training_labels)\n","testing_padded = np.array(testing_padded)\n","testing_labels = np.array(testing_labels)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ikaujB6yJwth"},"source":["# 2 Model"]},{"cell_type":"code","metadata":{"id":"FufaT4vlkiDE"},"source":["model = tf.keras.Sequential([\n","    tf.keras.layers.Embedding(vocab_size, embedding_dim, input_length=max_length),\n","    tf.keras.layers.GlobalAveragePooling1D(),\n","    tf.keras.layers.Dense(24, activation='relu'),\n","    tf.keras.layers.Dense(1, activation='sigmoid')\n","])\n","model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"XfDt1hmYkiys"},"source":["model.summary()\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"2DTKQFf1kkyc"},"source":["num_epochs = 30\n","history = model.fit(training_padded, training_labels, epochs=num_epochs, validation_data=(testing_padded, testing_labels), verbose=2)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"b0FtRUSKJ6Hv"},"source":["# 3 Plotting Loss and Accuracy"]},{"cell_type":"code","metadata":{"id":"2HYfBKXjkmU8"},"source":["import matplotlib.pyplot as plt\n","\n","\n","def plot_graphs(history, string):\n","  plt.plot(history.history[string])\n","  plt.plot(history.history['val_'+string])\n","  plt.xlabel(\"Epochs\")\n","  plt.ylabel(string)\n","  plt.legend([string, 'val_'+string])\n","  plt.show()\n","  \n","plot_graphs(history, \"accuracy\")\n","plot_graphs(history, \"loss\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"7SBdAZAenvzL"},"source":["reverse_word_index = dict([(value, key) for (key, value) in word_index.items()])\n","\n","def decode_sentence(text):\n","    return ' '.join([reverse_word_index.get(i, '?') for i in text])\n","\n","print(decode_sentence(training_padded[0]))\n","print(training_sentences[2])\n","print(labels[2])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"CFMQW8R3J_ZO"},"source":["# 4 Creating tsv data"]},{"cell_type":"code","metadata":{"id":"c9MqihtEkzQ9"},"source":["e = model.layers[0]\n","weights = e.get_weights()[0]\n","print(weights.shape) # shape: (vocab_size, embedding_dim)\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"LoBXVffknldU"},"source":["import io\n","\n","out_v = io.open('vecs.tsv', 'w', encoding='utf-8')\n","out_m = io.open('meta.tsv', 'w', encoding='utf-8')\n","for word_num in range(1, vocab_size):\n","  word = reverse_word_index[word_num]\n","  embeddings = weights[word_num]\n","  out_m.write(word + \"\\n\")\n","  out_v.write('\\t'.join([str(x) for x in embeddings]) + \"\\n\")\n","out_v.close()\n","out_m.close()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"U4eZ5HtVnnEE"},"source":["try:\n","  from google.colab import files\n","except ImportError:\n","  pass\n","else:\n","  files.download('vecs.tsv')\n","  files.download('meta.tsv')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"cG8-ArY-qDcz"},"source":["sentence = [\"granny starting to fear spiders in the garden might be real\", \"game of thrones season finale showing this sunday night\"]\n","sequences = tokenizer.texts_to_sequences(sentence)\n","padded = pad_sequences(sequences, maxlen=max_length, padding=padding_type, truncating=trunc_type)\n","print(model.predict(padded))"],"execution_count":null,"outputs":[]}]}